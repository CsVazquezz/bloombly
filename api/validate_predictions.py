#!/usr/bin/env python3
"""
Validation Script: Compare ML Model vs Kaggle Forecasts vs Actual 2024 Blooms
Generates accuracy metrics to prove model validity for hackathon evaluation
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import sys

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))


class PredictionValidator:
    def __init__(self):
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.actual_data = None
        self.ml_predictions = None
        self.kaggle_forecasts = None
        
    def load_actual_blooms(self, csv_path):
        """Load actual 2024 bloom observations (ground truth)"""
        print("\n[1/5] Loading actual 2024 bloom observations (GROUND TRUTH)...")
        
        df = pd.read_csv(csv_path)
        
        # Filter for 2024 actual blooms
        actual_2024 = df[df['year'] == 2024].copy()
        
        # Focus on Prunus (cherry) species
        actual_2024 = actual_2024[actual_2024['genus'] == 'Prunus'].copy()
        
        print(f"  ‚úì Loaded {len(actual_2024)} actual cherry bloom observations")
        print(f"  Date range: Day {actual_2024['day_of_year'].min():.0f} to Day {actual_2024['day_of_year'].max():.0f}")
        print(f"  Regions: {actual_2024['region'].unique().tolist()}")
        
        self.actual_data = actual_2024
        return actual_2024
    
    def load_ml_predictions(self, geojson_path):
        """Load ML model predictions"""
        print("\n[2/5] Loading ML model predictions...")
        
        with open(geojson_path, 'r') as f:
            geojson = json.load(f)
        
        predictions = []
        for feature in geojson['features']:
            props = feature['properties']
            coords = feature['geometry']['coordinates'][0][0]
            
            # Calculate center
            lons = [p[0] for p in coords]
            lats = [p[1] for p in coords]
            center_lon = sum(lons) / len(lons)
            center_lat = sum(lats) / len(lats)
            
            predictions.append({
                'name': props['city'],
                'latitude': center_lat,
                'longitude': center_lon,
                'predicted_day': props['bloom_day_of_year'],
                'predicted_date': props['predicted_bloom_date'],
                'country': props['country']
            })
        
        ml_df = pd.DataFrame(predictions)
        print(f"  ‚úì Loaded {len(ml_df)} ML predictions")
        
        self.ml_predictions = ml_df
        return ml_df
    
    def load_kaggle_forecasts(self, forecasts_path, places_path):
        """Load Kaggle forecasts for comparison"""
        print("\n[3/5] Loading Kaggle forecasts...")
        
        forecasts = pd.read_csv(forecasts_path)
        places = pd.read_csv(places_path)
        
        # Remove prefix from code
        places['code'] = places['code'].astype(str).str.lstrip('0').astype(int)
        
        # Merge
        kaggle = forecasts.merge(places, left_on='place_code', right_on='code', how='inner')
        
        # Filter for 2024
        kaggle['bloom_date'] = pd.to_datetime(kaggle['kaika_date'])
        kaggle_2024 = kaggle[kaggle['bloom_date'].dt.year == 2024].copy()
        kaggle_2024['predicted_day'] = kaggle_2024['bloom_date'].dt.dayofyear
        
        print(f"  ‚úì Loaded {len(kaggle_2024)} Kaggle forecasts")
        
        self.kaggle_forecasts = kaggle_2024[['spot_name', 'lat', 'lon', 'predicted_day', 'bloom_date']]
        return self.kaggle_forecasts
    
    def find_nearest_actual(self, pred_lat, pred_lon, actual_df, max_distance_deg=5.0):
        """Find nearest actual bloom observation to a prediction"""
        # Calculate distances
        actual_df = actual_df.copy()
        actual_df['distance'] = np.sqrt(
            (actual_df['latitude'] - pred_lat)**2 + 
            (actual_df['longitude'] - pred_lon)**2
        )
        
        # Find closest within max distance
        closest = actual_df.nsmallest(1, 'distance')
        
        if len(closest) > 0 and closest.iloc[0]['distance'] <= max_distance_deg:
            return closest.iloc[0]
        return None
    
    def validate_predictions(self):
        """Compare predictions against actual blooms"""
        print("\n[4/5] Validating predictions against actual blooms...")
        
        results = {
            'ml_model': {'matches': [], 'errors': [], 'predictions': []},
            'kaggle': {'matches': [], 'errors': [], 'predictions': []}
        }
        
        # Validate ML predictions
        print("\n  Validating ML model predictions...")
        for idx, pred in self.ml_predictions.iterrows():
            actual = self.find_nearest_actual(
                pred['latitude'], 
                pred['longitude'], 
                self.actual_data,
                max_distance_deg=2.0  # Within ~200km
            )
            
            if actual is not None:
                error = abs(pred['predicted_day'] - actual['day_of_year'])
                results['ml_model']['matches'].append({
                    'location': pred['name'],
                    'predicted': pred['predicted_day'],
                    'actual': actual['day_of_year'],
                    'error_days': error,
                    'distance_deg': actual['distance']
                })
                results['ml_model']['errors'].append(error)
                print(f"    ‚úì {pred['name']}: Predicted Day {pred['predicted_day']:.0f}, Actual Day {actual['day_of_year']:.0f}, Error: {error:.1f} days")
            else:
                print(f"    ‚ö† {pred['name']}: No nearby actual bloom found")
            
            results['ml_model']['predictions'].append(pred['predicted_day'])
        
        # Validate Kaggle (sample for comparison)
        print("\n  Validating Kaggle forecasts (sample)...")
        kaggle_sample = self.kaggle_forecasts.sample(min(100, len(self.kaggle_forecasts)))
        
        for idx, pred in kaggle_sample.iterrows():
            actual = self.find_nearest_actual(
                pred['lat'], 
                pred['lon'], 
                self.actual_data,
                max_distance_deg=0.5  # Closer match for dense Kaggle data
            )
            
            if actual is not None:
                error = abs(pred['predicted_day'] - actual['day_of_year'])
                results['kaggle']['matches'].append({
                    'location': pred['spot_name'],
                    'predicted': pred['predicted_day'],
                    'actual': actual['day_of_year'],
                    'error_days': error
                })
                results['kaggle']['errors'].append(error)
        
        print(f"    ‚úì Matched {len(results['kaggle']['matches'])} Kaggle forecasts to actual blooms")
        
        return results
    
    def generate_metrics(self, results):
        """Calculate accuracy metrics"""
        print("\n[5/5] Generating accuracy metrics...")
        
        metrics = {}
        
        # ML Model metrics
        if results['ml_model']['errors']:
            ml_errors = results['ml_model']['errors']
            metrics['ml_model'] = {
                'matches_found': len(results['ml_model']['matches']),
                'total_predictions': len(self.ml_predictions),
                'mean_absolute_error': np.mean(ml_errors),
                'median_error': np.median(ml_errors),
                'std_error': np.std(ml_errors),
                'max_error': np.max(ml_errors),
                'min_error': np.min(ml_errors),
                'rmse': np.sqrt(np.mean([e**2 for e in ml_errors])),
                'within_7_days': sum(1 for e in ml_errors if e <= 7) / len(ml_errors) * 100,
                'within_14_days': sum(1 for e in ml_errors if e <= 14) / len(ml_errors) * 100
            }
        
        # Kaggle metrics
        if results['kaggle']['errors']:
            kaggle_errors = results['kaggle']['errors']
            metrics['kaggle'] = {
                'matches_found': len(results['kaggle']['matches']),
                'sample_size': len(results['kaggle']['matches']),
                'mean_absolute_error': np.mean(kaggle_errors),
                'median_error': np.median(kaggle_errors),
                'std_error': np.std(kaggle_errors),
                'max_error': np.max(kaggle_errors),
                'min_error': np.min(kaggle_errors),
                'rmse': np.sqrt(np.mean([e**2 for e in kaggle_errors])),
                'within_7_days': sum(1 for e in kaggle_errors if e <= 7) / len(kaggle_errors) * 100,
                'within_14_days': sum(1 for e in kaggle_errors if e <= 14) / len(kaggle_errors) * 100
            }
        
        return metrics
    
    def print_report(self, results, metrics):
        """Print validation report"""
        print("\n" + "=" * 80)
        print("VALIDATION REPORT: ML MODEL vs KAGGLE vs ACTUAL 2024 BLOOMS")
        print("=" * 80)
        
        print("\nüìä GROUND TRUTH DATA:")
        print(f"  Total actual 2024 cherry blooms: {len(self.actual_data)}")
        print(f"  Date range: Day {self.actual_data['day_of_year'].min():.0f} - Day {self.actual_data['day_of_year'].max():.0f}")
        
        print("\n" + "=" * 80)
        print("üå∏ ML MODEL PERFORMANCE")
        print("=" * 80)
        
        if 'ml_model' in metrics:
            m = metrics['ml_model']
            print(f"\nPredictions validated: {m['matches_found']}/{m['total_predictions']}")
            print(f"\nAccuracy Metrics:")
            print(f"  Mean Absolute Error (MAE):  {m['mean_absolute_error']:.2f} days")
            print(f"  Median Error:               {m['median_error']:.2f} days")
            print(f"  Root Mean Square Error:     {m['rmse']:.2f} days")
            print(f"  Standard Deviation:         {m['std_error']:.2f} days")
            print(f"  Error Range:                {m['min_error']:.2f} - {m['max_error']:.2f} days")
            print(f"\nPrediction Accuracy:")
            print(f"  Within ¬±7 days:             {m['within_7_days']:.1f}%")
            print(f"  Within ¬±14 days:            {m['within_14_days']:.1f}%")
            
            print(f"\nüìç Detailed Matches:")
            for match in results['ml_model']['matches']:
                print(f"  {match['location']:20} ‚Üí Predicted: Day {match['predicted']:3.0f} | "
                      f"Actual: Day {match['actual']:3.0f} | Error: {match['error_days']:4.1f} days")
        
        print("\n" + "=" * 80)
        print("üìò KAGGLE FORECAST PERFORMANCE")
        print("=" * 80)
        
        if 'kaggle' in metrics:
            k = metrics['kaggle']
            print(f"\nSample validated: {k['matches_found']} forecasts")
            print(f"\nAccuracy Metrics:")
            print(f"  Mean Absolute Error (MAE):  {k['mean_absolute_error']:.2f} days")
            print(f"  Median Error:               {k['median_error']:.2f} days")
            print(f"  Root Mean Square Error:     {k['rmse']:.2f} days")
            print(f"  Standard Deviation:         {k['std_error']:.2f} days")
            print(f"\nPrediction Accuracy:")
            print(f"  Within ¬±7 days:             {k['within_7_days']:.1f}%")
            print(f"  Within ¬±14 days:            {k['within_14_days']:.1f}%")
        
        print("\n" + "=" * 80)
        print("üéØ COMPARISON SUMMARY")
        print("=" * 80)
        
        if 'ml_model' in metrics and 'kaggle' in metrics:
            ml_mae = metrics['ml_model']['mean_absolute_error']
            kaggle_mae = metrics['kaggle']['mean_absolute_error']
            
            print(f"\nMean Absolute Error:")
            print(f"  ML Model: {ml_mae:.2f} days")
            print(f"  Kaggle:   {kaggle_mae:.2f} days")
            
            if ml_mae < kaggle_mae:
                improvement = ((kaggle_mae - ml_mae) / kaggle_mae) * 100
                print(f"\n‚úÖ ML Model is {improvement:.1f}% MORE ACCURATE than Kaggle!")
            elif ml_mae > kaggle_mae:
                difference = ((ml_mae - kaggle_mae) / kaggle_mae) * 100
                print(f"\n‚ö†Ô∏è  Kaggle is {difference:.1f}% more accurate than ML Model")
            else:
                print(f"\n‚û°Ô∏è  ML Model and Kaggle have similar accuracy")
            
            print(f"\nML Model Advantages:")
            print(f"  ‚úÖ Uses satellite NDVI data")
            print(f"  ‚úÖ Uses environmental features (soil, GDD, photoperiod)")
            print(f"  ‚úÖ Global coverage (not just Japan)")
            print(f"  ‚úÖ Trained on 12 years of data (2013-2025)")
        
        print("\n" + "=" * 80)
        
        return metrics
    
    def save_report(self, results, metrics, output_path):
        """Save validation report to JSON"""
        report = {
            'validation_date': datetime.now().isoformat(),
            'ground_truth': {
                'total_observations': len(self.actual_data),
                'date_range': f"Day {self.actual_data['day_of_year'].min():.0f} - Day {self.actual_data['day_of_year'].max():.0f}"
            },
            'ml_model': {
                'metrics': metrics.get('ml_model', {}),
                'matches': results['ml_model']['matches']
            },
            'kaggle': {
                'metrics': metrics.get('kaggle', {}),
                'sample_size': len(results['kaggle']['matches'])
            }
        }
        
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\n‚úì Validation report saved to: {output_path}")


def main():
    print("=" * 80)
    print("PREDICTION VALIDATION - ML MODEL vs KAGGLE vs ACTUAL BLOOMS")
    print("=" * 80)
    
    validator = PredictionValidator()
    script_dir = validator.script_dir
    
    # Paths
    actual_csv = os.path.join(script_dir, '../data/processed/bloom_features_ml.csv')
    ml_predictions = os.path.join(script_dir, '../data/geojson/ml_predictions_2024.geojson')
    kaggle_forecasts = os.path.join(script_dir, '../data/raw/cherry_blossom_forecasts.csv')
    kaggle_places = os.path.join(script_dir, '../data/raw/cherry_blossom_places.csv')
    output_report = os.path.join(script_dir, '../data/validation_report_2024.json')
    
    # Load data
    validator.load_actual_blooms(actual_csv)
    validator.load_ml_predictions(ml_predictions)
    validator.load_kaggle_forecasts(kaggle_forecasts, kaggle_places)
    
    # Validate
    results = validator.validate_predictions()
    metrics = validator.generate_metrics(results)
    
    # Print report
    validator.print_report(results, metrics)
    
    # Save report
    validator.save_report(results, metrics, output_report)
    
    print("\n" + "=" * 80)
    print("‚úì VALIDATION COMPLETE")
    print("=" * 80)
    print("\nFor hackathon evaluation, highlight:")
    print("  1. Mean Absolute Error (MAE) - lower is better")
    print("  2. Percentage within ¬±7 days - higher is better")
    print("  3. Use of advanced features (NDVI, soil, GDD)")
    print("  4. Global coverage vs Japan-only")


if __name__ == "__main__":
    main()
